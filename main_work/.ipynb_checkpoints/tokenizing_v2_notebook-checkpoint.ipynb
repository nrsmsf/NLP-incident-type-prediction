{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "peaceful-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizing_v2 import cleaning_ru\n",
    "from tokenizing_v2 import lemmatize_ru\n",
    "import langdetect\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dimensional-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = \"../data/04-01_06-30sDate_80.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(my_file, dtype=str)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаляем ненужные столбцы и пустые поля\n",
    "\n",
    "df.dropna(subset = [\"Description\"], inplace=True)\n",
    "df.dropna(subset = [\"Detailed_Decription\"], inplace=True)\n",
    "df.fillna('0')\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tropical-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12892, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fulltext\"] = df[\"Description\"] + \" \" + df[\"Detailed_Decription\"]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "considered-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем колонку с определенным языком для каждой строки\n",
    "df[\"lang\"] = df['fulltext'].apply(langdetect.detect)\n",
    "#и удаляем ненужные столбцы еще раз\n",
    "df.drop(columns=['Description', 'Detailed_Decription'\n",
    "                ], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "micro-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ru = df[df.lang == 'ru']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ru.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "korean-british",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_insignificant_parts(\n",
    "        text: str, signatures: List[Tuple[str, ...]],\n",
    "        trash_lines: List[Tuple[str, ...]]\n",
    "    ) -> Tuple[List[str], int]:\n",
    "    \"\"\"\n",
    "    Cleans the string from junk and returns clean long lines and text length\n",
    "\n",
    "    :param text: input text\n",
    "    :param signatures: lines that show that the letter is over,\n",
    "        such as \"Best regards\"\n",
    "    :param trash_lines: lines that carry not useful info like\n",
    "        \"Thank you\"\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    good_lines = list()\n",
    "\n",
    "    lines = extract_lines(text)\n",
    "\n",
    "    for line in lines:\n",
    "        words = extract_words(line)\n",
    "        if words in signatures:\n",
    "            break\n",
    "        if words in trash_lines:\n",
    "            continue\n",
    "        good_lines.append(line)\n",
    "        total_words += len(words)\n",
    "\n",
    "    return good_lines\n",
    "\n",
    "\n",
    "def load_filter_list(path: str) -> List[Tuple[str, ...]]:\n",
    "    with open(path) as f:\n",
    "        filter_list = json.load(f)\n",
    "    return [tuple(l) for l in filter_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sufficient-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "trash_lines_loaded = load_filter_list(\"../trash_lines.json\")\n",
    "signatures_loaded = load_filter_list(\"../signatures.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prescription-queue",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trashline_arr = []\n",
    "for x in df_ru['fulltext']:\n",
    "    without_row_str = remove_insignificant_parts(x, signatures_loaded, trash_lines_loaded)\n",
    "    joined = \" \".join(without_row_str)\n",
    "    trashline_arr.append(joined)\n",
    "    \n",
    "#trashline_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "trashlined_df = df_ru.assign(trashlined=trashline_arr)\n",
    "trashlined_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "miniature-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "trashlined_df.to_csv('new_inc_and_trashlined.csv', index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "binding-polyester",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_arr = []\n",
    "for x in trashlined_df['trashlined']:\n",
    "    tokenized_row_str = lemmatize_ru(x)\n",
    "    tokenized_arr.append(tokenized_row_str)\n",
    "    \n",
    "#tokenized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_df = trashlined_df.assign(tokenized=tokenized_arr)\n",
    "tokened_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unauthorized-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_df.to_csv('new_inc_and_tokened.csv', index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "banner-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = []\n",
    "for x in tokened_df['tokenized']:\n",
    "    tokenized_row_str = cleaning_ru(x)\n",
    "    cleaned_texts.append(tokenized_row_str)\n",
    "#cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_df = tokened_df.assign(cleanedtext=cleaned_texts)\n",
    "tokened_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "understanding-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_df.to_csv('new_tokened_and_cleaned.csv', index=False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
